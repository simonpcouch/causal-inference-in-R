{
  "hash": "40176970259b6631c8afad838d175ef4",
  "result": {
    "markdown": "# The whole game: mosquito nets and malaria\n\n\n\n\n\nIn this chapter, we'll analyze data using techniques we learn in this book.\nWe'll play the [whole game](https://www.gse.harvard.edu/news/uk/09/01/education-bat-seven-principles-educators) of causal analysis using a few key steps:\n\n1.  Specify a causal question\n2.  Draw our assumptions using a causal diagram\n3.  Model our assumptions\n4.  Diagnose our models\n5.  Estimate the causal effect\n6.  Conduct sensitivity analysis on the effect estimate\n\nWe'll focus on the broader ideas behind each step and what they look like all together; however, we don't expect you to fully digest each idea. We'll spend the rest of the book taking up each step in detail.\n\n## Specify a causal question\n\nIn this guided exercise, we'll attempt to answer a causal question: does using a bed net reduce the risk of malaria?\n\nMalaria remains a serious public health issue.\nAdditionally, while malaria incidence has decreased since 2000, 2020 and the COVID-19 pandemic saw an increase in cases and deaths due primarily to service interruption [@worldma].\nAbout 86% of malaria deaths occurred in 29 countries.\nStill, nearly half of all malaria deaths occurred in just six countries: Nigeria (27%), the Democratic Republic of the Congo (12%), Uganda (5%), Mozambique (4%), Angola (3%), and Burkina Faso (3%).\nMost of these deaths occurred in children under 5 [@mosquito].\nMalaria also poses severe health risks to pregnant women and worsens birth outcomes, including early delivery and low birth weight.\n\nBed nets prevent morbidity and mortality due to malaria by providing a barrier against infective bites by the chief host of malaria parasites, the mosquito.\nHumans have used bed nets since ancient times.\nHerodotus, the 5th century BC Greek author of *The Histories*, observed Egyptians using their fishing nets as bed nets:\n\n> Against the gnats, which are very abundant, they have contrived as follows:---those who dwell above the fen-land are helped by the towers, to which they ascend when they go to rest; for the gnats by reason of the winds are not able to fly up high: but those who dwell in the fen-land have contrived another way instead of the towers, and this is it:---every man of them has got a casting net, with which by day he catches fish, but in the night he uses it for this purpose, that is to say he puts the casting-net round about the bed in which he sleeps, and then creeps in under it and goes to sleep: and the gnats, if he sleeps rolled up in a garment or a linen sheet, bite through these, but through the net they do not even attempt to bite [@thehist].\n\nMany modern nets are also treated with insecticide, dating back to Russian soldiers in World War II [@nevill1996], although some people still use them as fishing nets [@gettleman2015].\n\nIt's easy to imagine a randomized trial that deals with this question: participants in a study are randomly assigned to use a bed net, and we follow them over time to see if there is a difference in malaria risk between groups.\nRandomization is often the best way to estimate a causal effect of an intervention because it reduces the number of assumptions we need to make for that estimate to be valid (we discussed the assumptions we need to make for causal inference in @sec-causal-question).\nIn particular, randomization addresses confounding very well, accounting for confounders about which we may not even know.\n\nSeveral landmark trials have studied the effects of bed net use on malaria risk, with several essential studies in the 1990s.\nA 2004 meta-analysis found that insecticide-treated nets reduced childhood mortality by 17%, malarial parasite prevalence by 13%, and cases of uncomplicated and severe malaria by about 50% (compared to no nets) [@lengeler2004].\nSince the World Health Organization began recommending insecticide-treated nets, insecticide resistance has been a big concern.\nStill, a follow-up analysis of trials found that it has yet to impact the public health benefits of bed nets [@pryce2018].\n\nTrials have also been influential in determining the economics of bed net programs.\nFor instance, one trial compared free net distribution versus a cost-share program (where participants pay a subsidized fee for nets).\nThe study's authors found that net uptake was similar between the groups and that free net distribution---because it was easier to access--saved more lives, and was cheaper per life saved than the cost-sharing program [@cohen2010].\n\nThere are several reasons we might not be able to conduct a randomized trial, including ethics, cost, and time.\nWe have substantial, robust evidence in favor of bed net use.\nStill, let's consider some conditions where observational causal inference helps answer questions about bed nets and malaria prevention.\n\n-   Imagine we are at a time before trials on this subject, and let's say people have started to use bed nets for this purpose on their own.\n    Our goal may still be to conduct a randomized trial, but we can answer questions more quickly with observed data.\n    In addition, this study's results might guide trials' design or intermediary policy suggestions.\n\n-   Sometimes, it is also not ethical to conduct a trial.\n    An example of this in malaria research is a question that arose in the study of bed net effectiveness: does malaria control in early childhood result in delayed immunity to the disease, resulting in severe malaria or death later in life?\n    Since we now know bed net use is very effective, *withholding* nets would be unethical.\n    A recent observational study found that the benefits of bed net use in childhood on all-cause mortality persist into adulthood [@mosquito].\n\n-   We may also want to estimate a different effect or the effect for another population than in previous trials.\n    For example, both randomized and observational studies helped us better understand that insecticide-based nets improve malaria resistance in the entire community, not just among those who use nets, so long as net usage is high enough [@howard2000; @hawley2003].\n\nAs we saw in @sec-causal-question and we'll see in @sec-g-comp, the causal inference techniques that we'll discuss in this book are often beneficial even when we're able to randomize.\n\nWhen we conduct an observational study, it's still helpful to think through the randomized trial we would run were it possible.\nThe trial we're trying to emulate in this causal analysis is the *target trial.* Considering the target trial helps us make our causal question more accurate.\nLet's consider the causal question posed earlier: does using a bed net (a mosquito net) reduce the risk of malaria?\nThis question is relatively straightforward, but it is still vague.\nIn conducting an analysis, we'll need to address several key questions:\n\n-   What do we mean by \"bed net\"?\n    There are several types of nets: untreated bed nets, insecticide-treated bed nets, and newer long-lasting insecticide-treated bed nets.\n\n-   Risk compared to what?\n    Are we, for instance, comparing insecticide-treated bed nets to *no* net?\n    Untreated nets?\n    Or are we comparing a new type of net, like long-lasting insecticide-treated bed nets, to nets that are already in use?\n\n-   Risk as defined by what?\n    Whether or not a person contracted malaria?\n    Whether a person died of malaria?\n\n-   Risk among whom?\n    What is the population to which we're trying to apply this knowledge?\n    Who is it practical to include in our study?\n    Who might we need to exclude?\n\nWe will use simulated data to answer a more specific question: Does using insecticide-treated bed nets decrease the risk of contracting malaria?\nIn this particular data, [simulated by Dr. Andrew Heiss](https://evalsp21.classes.andrewheiss.com/example/matching-ipw/#program-background):\n\n> researchers are interested in whether using mosquito nets decreases an individual's risk of contracting malaria.\n> They have collected data from 1,752 households in an unnamed country and have variables related to environmental factors, individual health, and household characteristics.\n> The data is **not experimental**---researchers have no control over who uses mosquito nets, and individual households make their own choices over whether to apply for free nets or buy their own nets, as well as whether they use the nets if they have them.\n\nBecause we're using simulated data, we'll have direct access to a variable that measures the likelihood of contracting malaria, something we wouldn't likely have in real life.\nWe'll stick with this measure because we know the actual effect size.\nWe'll use simulated data, `net_data`, from the {causalworkshop} package (TODO: move this to causaldata?), which includes ten variables:\n\n`id`\n\n:   an ID variable\n\n`net` and `net_num`\n\n:   a binary variable indicating if the participant used a net (1) or didn't use a net (0), as a factor\n\n`malaria_risk`\n\n:   risk of malaria scale ranging from 0-100\n\n`income`\n\n:   weekly income, measured in dollars\n\n`health`\n\n:   a health score scale ranging from 0--100\n\n`household`\n\n:   number of people living in the household\n\n`eligible`\n\n:   a binary variable indicating if the household is eligible for the free net program.\n\n`temperature`\n\n:   the average temperature at night, in Celsius\n\n`resistance`\n\n:   Insecticide resistance of local mosquitoes.\n    A scale of 0--100, with higher values indicating higher resistance.\n\nThe distribution of malaria risk appears to be quite different by net usage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(parsnip)\nlibrary(causalworkshop)\n\nnet_data <- net_data %>% mutate(net = as.factor(net))\nnet_data_full <- net_data_full %>% mutate(net = as.factor(net))\n\nnet_data |>\n  ggplot(aes(malaria_risk, fill = net)) +\n  geom_density(color = NA, alpha = .8)\n```\n\n::: {.cell-output-display}\n![A density plot of malaria risk for those who did and did not use nets. The risk of malaria is lower for those who use nets.](chapter-02_files/figure-html/fig-malaria-risk-density-1.png){#fig-malaria-risk-density width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nIn @fig-malaria-risk-density, the density of those who used nets is to the left of those who did not use nets.\nThe mean difference in malaria risk is about 16.4, suggesting net use might be protective against malaria.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet_data |>\n  group_by(net) |>\n  summarize(malaria_risk = mean(malaria_risk))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  net   malaria_risk\n  <fct>        <dbl>\n1 FALSE         43.9\n2 TRUE          27.5\n```\n:::\n:::\n\n\nAnd that's what we see with simple linear regression, as well, as we would expect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n\nlinear_reg() |>\n  fit(malaria_risk ~ net, data = net_data) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     43.9     0.377     116.  0       \n2 netTRUE        -16.4     0.741     -22.1 1.10e-95\n```\n:::\n:::\n\n\n## Draw our assumptions using a causal diagram\n\nThe problem that we face is that other factors may be responsible for the effect we're seeing.\nIn this example, we'll focus on confounding: a common cause of net usage and malaria will bias the effect we see unless we account for it somehow.\nOne of the best ways to determine which variables we need to account for is to use a causal diagram.\nThese diagrams, also called causal directed acyclic graphs (DAGs), visualize the assumptions that we're making about the causal relationships between the exposure, outcome, and other variables we think might be related.\n\nHere's the DAG that we're proposing for this question.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A proposed causal diagram of the effect of bed net use on malaria. This directed acyclic graph (DAG) states our assumption that bed net use causes a reduction in malaria risk. It also says that we assume: malaria risk is impacted by net usage, income, health, temperature, and insecticide resistance; net usage is impacted by income, health, temperature, eligibility for the free net program, and the number of people in a household; eligibility for the free net programs is impacted by income and the number of people in a household; and health is impacted by income.](chapter-02_files/figure-html/fig-net-data-dag-1.png){#fig-net-data-dag width=672}\n:::\n:::\n\n\n(We'll explore how to create and analyze DAGs in R in @sec-dags).\n\nIn DAGs, each point represents a variable, and each arrow represents a cause.\nIn other words, this diagram declares what we think the causal relationships are between these variables.\nIn @fig-net-data-dag, we're saying that we believe:\n\n-   Malaria risk is causally impacted by net usage, income, health, temperature, and insecticide resistance.\n-   Net usage is causally impacted by income, health, temperature, eligibility for the free net program, and the number of people in a household.\n-   Eligibility for the free net programs is determined by income and the number of people in a household.\n-   Health is causally impacted by income.\n\nYou may agree or disagree with some of these assertions.\nThat's a good thing!\nLaying bare our assumptions allows us to consider the scientific credibility of our analysis.\nAnother benefit of using DAGs is that, thanks to their mathematics, we can determine precisely the subset of variables we need to account for if we assume this DAG is correct.\n\n::: callout-tip\n## Assembling DAGs\n\nIn this exercise, we're providing you with a reasonable DAG based on knowledge of how the data were generated.\nIn real life, setting up a DAG is a challenge requiring deep thought, domain expertise, and (often) collaboration between several experts.\n:::\n\nThe chief problem we're dealing with is that, when we analyze the data we're working with, we see the impact of net usage on malaria risk *and of all these other relationships*.\nIn DAG terminology, we have more than one open causal pathway.\nIf this DAG is correct, we have *eight* causal pathways: the path between net usage and malaria risk and seven other *confounding* pathways.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![In the proposed DAG, there are eight open pathways that contribute to the causal effect seen in the naive regression: the true effect (in green) of net usage on malaria risk and seven other confounding pathways (in orange). The naive estimate is wrong because it is a composite of all these effects.](chapter-02_files/figure-html/fig-net-data-confounding-1.png){#fig-net-data-confounding width=1344}\n:::\n:::\n\n\nWhen we calculate a naive linear regression that only includes net usage and malaria risk, the effect we see is incorrect because the seven other confounding pathways in @fig-net-data-confounding distort it.\nIn DAG terminology, we need to *block* these open pathways that distort the causal estimate we're after.\n(We can block paths through several techniques, including stratification, matching, weighting, and more. We'll see several methods throughout the book.) Luckily, by specifying a DAG, we can precisely determine the variables we need to control for.\nFor this DAG, we need to control for three variables: health, income, and temperature.\nThese three variables are a *minimal adjustment set*, the minimum set (or sets) of variables you need to block all confounding pathways.\nWe'll discuss adjustment sets further in @sec-dags.\n\n## Model our assumptions\n\nWe'll use a technique called Inverse Probability Weighting (IPW) to control for these variables, which we'll discuss in detail in @sec-using-ps.\nWe'll use logistic regression to predict the probability of treatment---the propensity score.\nThen, we'll calculate inverse probability weights to apply to the linear regression model we fit above.\nThe propensity score model includes the exposure---net use---as the dependent variable and the minimal adjustment set as the independent variables.\n\n::: callout-tip\n## Modeling the functional form\n\nGenerally speaking, we want to lean on domain expertise and good modeling practices to fit the propensity score model.\nFor instance, we may want to allow continuous confounders to be non-linear using splines, or we may want to add essential interactions between confounders.\nBecause these are simulated data, we know we don't need these extra parameters (so we'll skip them), but in practice, you often do.\nWe'll discuss this more in @sec-using-ps.\n:::\n\nThe propensity score model is a logistic regression model with the formula `net ~ income + health + temperature`, which predicts the probability of bed net usage based on the confounders income, health, and temperature.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npropensity_model <- logistic_reg() %>%\n  fit(\n    net ~ income + health + temperature,\n    data = net_data\n  )\n\n# the first six propensity scores\nhead(predict(propensity_model, net_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 1\n  .pred_class\n  <fct>      \n1 FALSE      \n2 FALSE      \n3 FALSE      \n4 FALSE      \n5 FALSE      \n6 FALSE      \n```\n:::\n:::\n\n\nWe can use propensity scores to control for confounding in various ways.\nIn this example, we'll focus on weighting.\nIn particular, we'll compute the inverse probability weight for the *average treatment effect* (ATE).\nThe ATE represents a particular causal question: what if *everyone* in the study used bed nets vs. what if *no one* in the study used bed nets?\n\nTo calculate the ATE, we'll use the broom and propensity packages.\nbroom's `augment()` function extracts prediction-related information from the model and joins it to the data.\npropensity's `wt_ate()` function calculates the inverse probability weight given the propensity score and exposure.\n\nFor inverse probability weighting, the ATE weight is the probability of receiving the treatment you actually received.\nIn other words, if you used a bed net, the ATE weight is the probability that you used a net, and if you did *not* use a net, it is the probability that you did not use a net.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(propensity)\nnet_data_wts <- propensity_model |>\n  augment(net_data) |>\n  # .fitted is the value predicted by the model\n  # for a given observation\n  mutate(\n    .fitted = .pred_TRUE,\n    wts = wt_ate(.fitted, net, .treated = \"TRUE\")\n  )\n\nnet_data_wts |>\n  select(net, .fitted, wts) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  net   .fitted   wts\n  <fct>   <dbl> <dbl>\n1 FALSE   0.246  1.33\n2 FALSE   0.218  1.28\n3 FALSE   0.323  1.48\n4 FALSE   0.231  1.30\n5 FALSE   0.279  1.39\n6 FALSE   0.306  1.44\n```\n:::\n:::\n\n\n`wts` represents the amount each observation will be up-weighted or down-weighted in the outcome model we will soon fit.\nFor instance, the first household used a bed net and had a predicted probability of 0.25.\nThat's a pretty low probability considering they did, in fact, use a net, so their weight is higher at 1.33.\nIn other words, this household will be up-weighted almost three times compared to the naive linear model we fit above.\nThe second household did *not* use a bed net; they're predicted probability of net use was 0.22 (or put differently, a predicted probability of *not* using a net of 0.78).\nThat's more in line with their observed value of `net`, but there's still some predicted probability of using a net, so their weight is 1.28.\n\n## Diagnose our models\n\nThe goal of propensity score weighting is to weight the population of observations such that the distribution of confounders is balanced between the exposure groups.\nHere's the distribution of the propensity score by group, created by `geom_mirror_histogram()` from the halfmoon package for assessing balance in propensity score models (as well as visualizing the pseudo-population the weights simulate):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(halfmoon)\nggplot(net_data_wts, aes(.fitted)) +\n  geom_mirror_histogram(\n    aes(fill = net),\n    bins = 50\n  ) +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Computation failed in `stat_mirror_count()`\nCaused by error in `compute_group()`:\n! unused argument (width = NULL)\n```\n:::\n\n::: {.cell-output-display}\n![A mirrored histogram of the propensity scores of those who used nets (top, blue) versus those who who did not use nets (bottom, light orange). The range of propensity scores is similar between groups, with those who used nets slightly to the left of those who didn't, but the shapes of the distribution are different.](chapter-02_files/figure-html/fig-mirror-histogram-net-data-unweighted-1.png){#fig-mirror-histogram-net-data-unweighted width=672}\n:::\n:::\n\n\nThe weighted propensity score creates a pseudo-population where the distributions are much more similar:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(net_data_wts, aes(.fitted)) +\n  geom_mirror_histogram(\n    aes(group = net),\n    bins = 50\n  ) +\n  geom_mirror_histogram(\n    aes(fill = net, weight = wts),\n    bins = 50,\n    alpha = .5\n  ) +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Computation failed in `stat_mirror_count()`\nComputation failed in `stat_mirror_count()`\nCaused by error in `compute_group()`:\n! unused argument (width = NULL)\n```\n:::\n\n::: {.cell-output-display}\n![A mirrored histogram of the propensity scores of those who used nets (top, blue) versus those who who did not use nets (bottom, light orange). The shaded region represents the unweighted distribution, and the colored region represents the weighted distributions. The ATE weights up-weight the groups to be similar in range and shape of the distribution of propensity scores.](chapter-02_files/figure-html/fig-mirror-histogram-net-data-weighted-1.png){#fig-mirror-histogram-net-data-weighted width=672}\n:::\n:::\n\n\nIn this example, the unweighted distributions are not awful---the shapes are fairly similar here---but the weighted distributions in @fig-mirror-histogram-net-data-weighted are much more similar.\n\n::: callout-caution\n## Unmeasured confounding\n\nPropensity score weighting and most other causal inference techniques only help with *observed* confounders---ones that we model correctly, at that.\nUnfortunately, we still may have unmeasured confounding, which we'll discuss below.\n\nRandomization is one causal inference technique that *does* deal with unmeasured confounding, one of the reasons it is so powerful.\n:::\n\nWe might also want to know how well-balanced the groups are by confounder.\nOne way to do this is to calculate the standardized mean differences (SMDs) for each confounder with and without weights.\nWe'll calculate the SMDs with `tidy_smd()` then plot them with `geom_love()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_df <- net_data_wts %>%\n  tidy_smd(\n  c(income, health, temperature),\n  .group = net,\n  .wts = wts\n)\n\nggplot(\n  plot_df,\n  aes(\n    x = abs(smd),\n    y = variable,\n    group = method,\n    color = method\n  )\n) +\n  geom_love()\n```\n\n::: {.cell-output-display}\n![A love plot representing the standardized mean differences (SMD) between exposure groups of three confounders: temperature, income, and health. Before weighting, there is considerable differences in the groups. After weighting, the confounders are much more balanced between groups.](chapter-02_files/figure-html/fig-love-plot-net-data-1.png){#fig-love-plot-net-data width=672}\n:::\n:::\n\n\nA standard guideline is that balanced confounders should have an SMD of less than 0.1 on the absolute scale.\n0.1 is just a rule of thumb, but if we follow it, the variables in @fig-love-plot-net-data are well-balanced after weighting (and unbalanced before weighting).\n\nBefore we apply the weights to the outcome model, let's check their overall distribution for extreme weights.\nExtreme weights can destabilize the estimate and variance in the outcome model, so we want to be aware of it.\nWe'll also discuss several other types of weights that are less prone to this issue in @sec-estimands.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet_data_wts |>\n  ggplot(aes(wts)) +\n  geom_density(fill = \"#CC79A7\", color = NA, alpha = 0.8)\n```\n\n::: {.cell-output-display}\n![A density plot of the average treatment effect (ATE) weights. The plot is skewed, with higher values towards 8. This may indicate a problem with the model, but the weights aren't so extreme to destabilize the variance of the estimate.](chapter-02_files/figure-html/fig-ate-density-net-data-1.png){#fig-ate-density-net-data width=672}\n:::\n:::\n\n\nThe weights in @fig-ate-density-net-data are skewed, but there are no outrageous values.\nIf we saw extreme weights, we might try trimming or stabilizing them, or consider calculating an effect for a different estimand, which we'll discuss in @sec-estimands.\nIt doesn't look like we need to do that here, however.\n\n## Estimate the causal effect\n\nWe're now ready to use the ATE weights to (attempt to) account for confounding in the naive linear regression model.\nFitting such a model is pleasantly simple in this case: we fit the same model as before but with `weights = wts`, which will incorporate the inverse probability weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  fit(\n    malaria_risk ~ net, \n    data = net_data_wts, \n    case_weights = importance_weights(net_data_wts$wts)\n  ) |>\n  tidy(conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 7\n  term        estimate std.e…¹ stati…²  p.value conf.…³\n  <chr>          <dbl>   <dbl>   <dbl>    <dbl>   <dbl>\n1 (Intercept)     42.7   0.442    96.7 0           41.9\n2 netTRUE        -12.5   0.624   -20.1 5.50e-81   -13.8\n# … with 1 more variable: conf.high <dbl>, and\n#   abbreviated variable names ¹​std.error, ²​statistic,\n#   ³​conf.low\n```\n:::\n:::\n\n\n\n\nThe estimate for the average treatment effect is -12.5 (95% CI -13.8, -11.3).\nUnfortunately, the confidence intervals we're using are wrong because they don't account for the dependence within the weights!\nGenerally, confidence intervals for propensity score weighted models will be too narrow unless we correct for this dependence.\nThe nominal coverage of the confidence intervals will thus be wrong (they aren't 95% CIs because their coverage is much lower than 95%) and may lead to misinterpretation.\n\nWe've got several ways to address this problem, which we'll discuss in detail in @sec-outcome-model, including the bootstrap, robust standard errors, and manually accounting for the dependence with empirical sandwich estimators.\nFor this example, we'll use the bootstrap, a flexible tool that calculates distributions of parameters using re-sampling.\nWe'll use the rsample package from the tidymodels ecosystem to work with bootstrap samples.\n\n::: callout-note\n## The Bootstrap\n\nThe bootstrap is a simple but flexible algorithm for calculating statistics using re-sampling with replacement.\nIt's handy when a closed-form solution doesn't exist to calculate something, as is commonly the case in causal inference (particularly for standard errors), and when we suspect the parametric calculations are not valid for a given situation.\n\nBootstrapping in R has a long tradition of writing functions to calculate the statistic of interest, starting with the classic boot package.\nThroughout the book, we'll use rsample, a more modern alternative for re-sampling, but generally, we start with writing a function to calculate the estimate we're interested in.\n\nLet's say we want to calculate `some_statistic()` for `this_data`.\nTo bootstrap for *R* samples, we:\n\n1.  Re-sample `this_data` with replacement.\n    The same row may appear multiple (or no) times in a given bootstrap sample, simulating the sampling process in the underlying population.\n\n    ``` r\n    indices <- sample(\n      # create a vector of indices:\n      # 1 through the number of rows\n      seq_len(nrow(this_data)), \n      # sample a vector indices \n      # that's the same length as `this_data`\n      size = nrow(this_data), \n      replace = TRUE\n    )\n    bootstrap_sample <- this_data[indices, ]\n    ```\n\n2.  Fit `some_statistic()` on the `bootstrap_sample`\n\n    ``` r\n    estimate <- some_statistic(bootstrap_sample)\n    ```\n\n3.  Repeat *R* times\n\nWe then end up with a distribution of `estimate`s, with which we can calculate population statistics, such as point estimates, standard errors, and confidence intervals.\n:::\n\nBecause the bootstrap is so flexible, we need to think carefully about the sources of uncertainty in the statistic we're calculating.\nIt might be tempting to write a function like this to fit the statistic we're interested in (the point estimate for `netTRUE`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rsample)\n\nfit_ipw_not_quite_rightly <- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  .df <- analysis(split)\n\n  # fit ipw model\n  linear_reg() %>%\n    fit(malaria_risk ~ net, data = .df, case_weights = .df$wts) |>\n    tidy()\n}\n```\n:::\n\n\nHowever, this function won't give us the correct confidence intervals because it treats the inverse probability weights as fixed values.\nThey're not, of course; we just estimated them using logistic regression!\nWe need to account for this uncertainty by bootstrapping the *entire modeling process*.\nFor every bootstrap sample, we need to fit the propensity score model, calculate the inverse probability weights, then fit the weighted outcome model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rsample)\n\nfit_ipw <- function(split, ...) {\n  # get bootstrapped data sample with `rsample::analysis()`\n  .df <- analysis(split)\n\n  # fit propensity score model\n  propensity_model <- logistic_reg() %>%\n    fit(\n      net ~ income + health + temperature,\n      data = .df\n    )\n\n  # calculate inverse probability weights\n  .df <- propensity_model |>\n    augment(.df) |>\n    mutate(wts = importance_weights(wt_ate(.pred_TRUE, net, .treated = \"TRUE\")))\n\n  # fit correctly bootstrapped ipw model\n  linear_reg() %>%\n    fit(malaria_risk ~ net, data = .df, case_weights = .df$wts) |>\n    tidy()\n}\n```\n:::\n\n\nNow that we know precisely how to calculate the estimate for each iteration let's create the bootstrapped dataset with rsample's `bootstraps()` function.\nThe `times` argument determines how many bootstrapped datasets to create; we'll do 1,000.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrapped_net_data <- bootstraps(\n  net_data,\n  times = 1000,\n  # required to calculate CIs later\n  apparent = TRUE\n)\n\nbootstrapped_net_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bootstrap sampling with apparent sample \n# A tibble: 1,001 × 2\n   splits             id           \n   <list>             <chr>        \n 1 <split [1752/657]> Bootstrap0001\n 2 <split [1752/658]> Bootstrap0002\n 3 <split [1752/634]> Bootstrap0003\n 4 <split [1752/639]> Bootstrap0004\n 5 <split [1752/611]> Bootstrap0005\n 6 <split [1752/648]> Bootstrap0006\n 7 <split [1752/644]> Bootstrap0007\n 8 <split [1752/616]> Bootstrap0008\n 9 <split [1752/675]> Bootstrap0009\n10 <split [1752/672]> Bootstrap0010\n# … with 991 more rows\n```\n:::\n:::\n\n\nThe result is a nested data frame: each `splits` object contains metadata that rsample uses to subset the bootstrap samples for each of the 1,000 samples.\nNext, we'll run `fit_ipw()` 1,000 times to create a distribution for `estimate`.\nAt its heart, the calculation we're doing is\n\n``` r\nfit_ipw(bootstrapped_net_data$splits[[n]])\n```\n\nWhere *n* is one of 1,000.\nWe'll use purrr's `map()` function to iterate across each `split` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nipw_results <- bootstrapped_net_data |>\n  mutate(boot_fits = map(splits, fit_ipw))\n\nipw_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bootstrap sampling with apparent sample \n# A tibble: 1,001 × 3\n   splits             id            boot_fits       \n   <list>             <chr>         <list>          \n 1 <split [1752/657]> Bootstrap0001 <tibble [2 × 5]>\n 2 <split [1752/658]> Bootstrap0002 <tibble [2 × 5]>\n 3 <split [1752/634]> Bootstrap0003 <tibble [2 × 5]>\n 4 <split [1752/639]> Bootstrap0004 <tibble [2 × 5]>\n 5 <split [1752/611]> Bootstrap0005 <tibble [2 × 5]>\n 6 <split [1752/648]> Bootstrap0006 <tibble [2 × 5]>\n 7 <split [1752/644]> Bootstrap0007 <tibble [2 × 5]>\n 8 <split [1752/616]> Bootstrap0008 <tibble [2 × 5]>\n 9 <split [1752/675]> Bootstrap0009 <tibble [2 × 5]>\n10 <split [1752/672]> Bootstrap0010 <tibble [2 × 5]>\n# … with 991 more rows\n```\n:::\n:::\n\n\nThe result is another nested data frame with a new column, `boot_fits`.\nEach element of `boot_fits` is the result of the IPW for the bootstrapped dataset.\nFor example, in the first bootstrapped data set, the IPW results were:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nipw_results$boot_fits[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     42.8     0.457      93.7 0       \n2 netTRUE        -12.3     0.646     -19.1 9.13e-74\n```\n:::\n:::\n\n\nNow we have a distribution of estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nipw_results |>\n  mutate(\n    estimate = map_dbl(\n      boot_fits,\n      # pull the `estimate` for `netTRUE` for each fit\n      \\(.fit) .fit |>\n        filter(term == \"netTRUE\") |>\n        pull(estimate)\n    )\n  ) |>\n  ggplot(aes(estimate)) +\n  geom_histogram(fill = \"#D55E00FF\", color = \"white\", alpha = 0.8)\n```\n\n::: {.cell-output-display}\n![\"A histogram of 1,000 bootstrapped estimates of the effect of net use on malaria risk. The spread of these estimates accounts for the dependency and uncertainty in the use of IPW weights.\"](chapter-02_files/figure-html/fig-bootstrap-estimates-net-data-1.png){#fig-bootstrap-estimates-net-data width=672}\n:::\n:::\n\n\n@fig-bootstrap-estimates-net-data gives a sense of the variation in `estimate`, but let's calculate 95% confidence intervals from the bootstrapped distribution using rsample's `int_t()` :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot_estimate <- ipw_results |>\n  # calculate T-statistic-based CIs\n  int_t(boot_fits) |>\n  filter(term == \"netTRUE\")\n\nboot_estimate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  term    .lower .estimate .upper .alpha .method  \n  <chr>    <dbl>     <dbl>  <dbl>  <dbl> <chr>    \n1 netTRUE  -13.3     -12.6  -11.6   0.05 student-t\n```\n:::\n:::\n\n\nNow we have a confounder-adjusted estimate with correct standard errors.\nThe estimate of the effect of *all* households using bed nets versus *no* households using bed nets on malaria risk is -12.6 (95% CI -13.3, -11.6).\nBed nets do indeed seem to reduce malaria risk in this study.\n\n## Conduct sensitivity analysis on the effect estimate\n\nWe've laid out a roadmap for taking observational data, thinking critically about the causal question we want to ask, identifying the assumptions we need to get there, then applying those assumptions to a statistical model.\nGetting the correct answer to the causal question relies on getting our assumptions more or less right.\nBut what if we're more on the less correct side?\n\nSpoiler alert: the answer we just calculated is **wrong**.\nAfter all that effort!\n\nWhen conducting a causal analysis, it's a good idea to use sensitivity analyses to test your assumptions.\nThere are many potential sources of bias in any study and many sensitivity analyses to go along with them; we'll focus on the assumption of no confounding.\n\nLet's start with a broad sensitivity analysis; then, we'll ask questions about specific unmeasured confounders.\nWhen we have less information about unmeasured confounders, we can use tipping point analysis to ask how much confounding it would take to tip my estimate to the null.\nIn other words, what would the strength of the unmeasured confounder have to be to explain our results away?\nThe tipr package is a toolkit for conducting sensitivity analyses.\nLet's examine the tipping point for an unknown, normally-distributed confounder.\nThe `tip_coef()` function takes an estimate (a beta coefficient from a regression model, or the upper or lower bound of the coefficient).\nIt further requires either the 1) scaled differences in means of the confounder between exposure groups or 2) effect of the confounder on the outcome.\nFor the estimate, we'll use `conf.high`, which is closer to 0 (the null), and ask: how much would the confounder have to affect malaria risk to have an unbiased upper confidence interval of 0?\nWe'll use tipr to calculate this answer for 5 scenarios, where the mean difference in the confounder between exposure groups is 1, 2, 3, 4, or 5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tipr)\ntipping_points <- tip_coef(boot_estimate$.upper, exposure_confounder_effect = 1:5)\n\ntipping_points |>\n  ggplot(aes(confounder_outcome_effect, exposure_confounder_effect)) +\n  geom_line(color = \"#009E73\", size = 1.1) +\n  geom_point(fill = \"#009E73\", color = \"white\", size = 2.5, shape = 21) +\n  labs(\n    x = \"Confounder-Outcome Effect\",\n    y = \"Scaled mean differences in\\n confounder between exposure groups\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in\nggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![A tipping point analysis under several confounding scenarios where the unmeasured confounder is a normally-distributed continuous variable. The line represents the strength of confounding necessary to tip the upper confidence interval of the causal effect estimate to 0. The x-axis represents the coefficient of the confounder-outcome relationship adjusted for the exposure and the set of measured confounders. The y-axis represents the scaled mean difference of the confounder between exposure groups.](chapter-02_files/figure-html/fig-tip-coef-net-1.png){#fig-tip-coef-net width=672}\n:::\n:::\n\n\nIf we had an unmeasured confounder where the standardized mean difference between exposure groups was 1, the confounder would need to decrease malaria risk by about -11.6.\nThat's pretty strong relative to other effects, but it may be feasible if we have an idea of something we might have missed.\nConversely, suppose the relationship between net use and the unmeasured confounder is very strong, with a mean scaled difference of 5.\nIn that case, the confounder-malaria relationship only needs to be 5.\nNow we have to consider: which of these scenarios are plausible given our domain knowledge and the effects we see in this analysis?\n\nNow let's consider a much more specific sensitivity analysis.\nSome ethnic groups, such as the Fulani, have a genetic resistance to malaria [@arama2015].\nLet's say that in our simulated data, an unnamed ethnic group shares this genetic resistance to malaria.\nFor historical reasons, bed net use in this fictional group is also very high.\nWe don't have this variable in `net_data`, but let's say we know from the literature that in this sample, we can estimate at:\n\n1.  People with this genetic resistance have, on average, about 10 lower malaria risk.\n2.  About 26% of people who use nets in our study have this genetic resistance.\n3.  About 5% of people who don't use nets have this genetic resistance.\n\nWith this amount of information, we can use tipr to adjust the estimates we calculated for the unmeasured confounder.\nWe'll use `adjust_coef_with_binary()` to calculate the adjusted estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadjusted_estimates <- boot_estimate |>\n  select(.estimate, .lower, .upper) |>\n  unlist() |>\n  adjust_coef_with_binary(\n    exposed_confounder_prev = 0.26,\n    unexposed_confounder_prev = 0.05,\n    confounder_outcome_effect = -10\n  )\n\nadjusted_estimates\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  effect_adjusted effect_observed exposure_co…¹ confo…²\n            <dbl>           <dbl>         <dbl>   <dbl>\n1          -10.5            -12.6          0.21     -10\n2          -11.2            -13.3          0.21     -10\n3           -9.50           -11.6          0.21     -10\n# … with abbreviated variable names\n#   ¹​exposure_confounder_effect,\n#   ²​confounder_outcome_effect\n```\n:::\n:::\n\n\nThe adjusted estimate for a situation where genetic resistance to malaria is a confounder is -10.5 (95% CI -11.2, -9.5).\n\nIn fact, these data were simulated with just such a confounder.\nThe true effect of net use on malaria is about -10, and the true DAG that generated these data is:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The true causal diagram for `net_data`. This DAG is identical to the one we proposed with one addition: genetic resistance to malaria causally reduces the risk of malaria and impacts net use. It's thus a confounder and a part of the minimal adjustment set required to get an unbiased effect estimate. In otherwords, by not including it, we've calculated the wrong effect.](chapter-02_files/figure-html/fig-net-data-true-dag-1.png){#fig-net-data-true-dag width=672}\n:::\n:::\n\n\n\n\nThe unmeasured confounder in @fig-net-data-true-dag is available in the dataset `net_data_full` as `genetic_resistance`.\nIf we recalculate the IPW estimate of the average treatment effect of nets on malaria risk, we get -10.2 (95% CI -11.1, -9.3), much closer to the actual answer of -10.\n\nWhat do you think?\nIs this estimate reliable?\nDid we do a good job addressing the assumptions we need to make for a causal effect, mainly that there is no confounding?\nHow might you criticize this model, and what would you do differently?\nOk, we know that -10 is the correct answer because the data are simulated, but in practice, we can never be sure, so we need to continue probing our assumptions until we're confident they are robust.\n<!-- TODO: Maybe use sickle cell as an example of a precision variable in the variable selection section later in the book. Interesting instance because sickle cell can't be downstream. Consider in the context of over adjustment. -->\n\nTo calculate this effect, we:\n\n1.  Specified a causal question (for the average treatment effect)\n2.  Drew our assumptions using a causal diagram (using DAGs)\n3.  Modeled our assumptions (using propensity score weighting)\n4.  Diagnosed our models (by checking confounder balance after weighting)\n5.  Estimated the causal effect (using inverse probability weighting)\n6.  Conducted sensitivity analysis on the effect estimate (using tipping point analysis)\n\nThroughout the rest of the book, we'll follow these broad steps in several examples from medicine, economics, and industry.\nWe'll dive more deeply into propensity score techniques, explore alternative methods for calculating causal effects, and, most importantly, make sure, over and over again, that the assumptions we're making are reasonable---even if we'll never know for sure.\n",
    "supporting": [
      "chapter-02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}